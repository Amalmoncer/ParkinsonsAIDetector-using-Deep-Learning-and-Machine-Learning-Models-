{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNmfZ437BdG+wz9WabHI6kP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tensorflow"],"metadata":{"id":"t1jpgQAZ6DfR"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":2,"metadata":{"id":"nWMngcQmqnBV","executionInfo":{"status":"ok","timestamp":1712514755447,"user_tz":-60,"elapsed":2454,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}}},"outputs":[],"source":["import pandas as pd\n","\n","df = pd.read_csv(\"/content/parkinsons.csv\")"]},{"cell_type":"code","source":["df.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcJJ7gFpAjHE","executionInfo":{"status":"ok","timestamp":1712517693798,"user_tz":-60,"elapsed":275,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"88d2c45a-c76b-4f77-86f6-63017507cbc5"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(195, 24)"]},"metadata":{},"execution_count":19}]},{"cell_type":"markdown","source":["#I. Préparez ldataset en sélectionnant les features et target -> Divisez le dataset en training set et de test set. -> Appliquer feature scaling"],"metadata":{"id":"Hz_kBqydB-u-"}},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","features = df.drop(['name', 'status'], axis=1) # Dropping the 'name' column as it's not relevant for prediction\n","target = df['status'] #'status' is the target variable you want to predict\n","\n","# Splitting the dataset (into training and test sets)\n","X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2,stratify=target,  random_state=42)\n","\n","#train_test_split(...): Splits the data into training and testing sets.\n","# test_size=0.2 means 20% for the test set, 80% for training.\n","# stratify=target ensures that the split maintains the same proportion of class labels in both sets.\n","# random_state=42 sets a seed for reproducibility.\n","\n","\n","# Standardizing the features\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# StandardScaler(): Creates a scaler object to standardize features.\n","# scaler.fit_transform(X_train): Fits the scaler to the training data and transforms it.\n","# scaler.transform(X_test): Transforms the testing data using the scaler fitted on the training data."],"metadata":{"id":"EhQvo-UR0fsD","executionInfo":{"status":"ok","timestamp":1712514981287,"user_tz":-60,"elapsed":274,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["# II. model training"],"metadata":{"id":"qHIt9BsP2gWp"}},{"cell_type":"markdown","source":["##1. Logistic Regression"],"metadata":{"id":"Gtod3b7V2k0T"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","# Initialize the Logistic Regression model\n","log_reg = LogisticRegression(random_state=42)\n","\n","# Train the model\n","log_reg.fit(X_train_scaled, y_train)\n","\n","# Predict on the test set\n","y_pred = log_reg.predict(X_test_scaled)\n","\n","# Evaluate the model\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","accuracy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XfO7qZrm1cBC","executionInfo":{"status":"ok","timestamp":1712514985525,"user_tz":-60,"elapsed":269,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"1a3935dd-6192-4c77-99e8-beee422d63cd"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9230769230769231"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","source":["##2. Support Vector Machines (SVM)"],"metadata":{"id":"amiYS-4k2xOJ"}},{"cell_type":"code","source":["from sklearn.svm import SVC\n","\n","# Initialize the Support Vector Machine model\n","svm_model = SVC(random_state=42)\n","\n","# Train the model\n","svm_model.fit(X_train_scaled, y_train)\n","\n","# Predict on the test set\n","y_pred_svm = svm_model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","\n","accuracy_svm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PARMhbQx2ySl","executionInfo":{"status":"ok","timestamp":1712515186201,"user_tz":-60,"elapsed":315,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"7fc5e88e-7b32-44ed-c533-75dffeb3476a"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9230769230769231"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["##3. Neural Networks model (**deep learning**)\n","\n","> Add blockquote\n","\n","\n"],"metadata":{"id":"T-_-bWKj3Itp"}},{"cell_type":"code","source":["from sklearn.neural_network import MLPClassifier\n","\n","# Initialize the Multi-Layer Perceptron (Neural Network) model\n","mlp_model = MLPClassifier(random_state=42, max_iter=1000)\n","\n","# Train the model\n","mlp_model.fit(X_train_scaled, y_train)\n","\n","# MLPClassifier(...): Creates a neural network classifier with a random seed for reproducibility and a maximum of 1000 iterations.\n","# mlp_model.fit(...): Trains the model on the scaled training data.\n","\n","\n","# Predict on the test set\n","y_pred_mlp = mlp_model.predict(X_test_scaled)\n","\n","# Evaluate the model\n","accuracy_mlp = accuracy_score(y_test, y_pred_mlp)\n","classification_rep_mlp = classification_report(y_test, y_pred_mlp)\n","\n","accuracy_mlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ma1uhI143KvZ","executionInfo":{"status":"ok","timestamp":1712515276112,"user_tz":-60,"elapsed":2539,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"da2da818-e261-4450-d2a7-1868cd2f32a5"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9487179487179487"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["*=> Neutral Network give higher accuracy than both the Logistic Regression and SVM models*"],"metadata":{"id":"NGNH3PB73fEY"}},{"cell_type":"markdown","source":["##4. Convolutional Neural Network (CNN) model (**Deep learning**)"],"metadata":{"id":"4GaKQaHx4WeZ"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Conv1D, Flatten, MaxPooling1D\n","import numpy as np\n","\n","# Reshaping the data for CNN\n","# CNN expects data in the shape (number of samples, number of features, 1)\n","X_train_cnn = np.expand_dims(X_train_scaled, axis=2)\n","X_test_cnn = np.expand_dims(X_test_scaled, axis=2)\n","\n","# Defining the CNN model\n","cnn_model = Sequential()\n","cnn_model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_cnn.shape[1], 1)))\n","cnn_model.add(MaxPooling1D(pool_size=2))\n","cnn_model.add(Flatten())\n","cnn_model.add(Dense(50, activation='relu'))\n","cnn_model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = cnn_model.fit(X_train_cnn, y_train, epochs=100, batch_size=32, verbose=0, validation_data=(X_test_cnn, y_test))\n","\n","# Evaluate the model\n","cnn_accuracy = cnn_model.evaluate(X_test_cnn, y_test, verbose=0)[1]\n","cnn_accuracy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u0YxjBvf6NQ3","executionInfo":{"status":"ok","timestamp":1712516052237,"user_tz":-60,"elapsed":18834,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"f5d67959-c0cc-4181-abb7-1f78e1b78851"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9230769276618958"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["##5. Recurrent Neural Network (RNN) model (**`deep learning`**)\n","\n","---\n","\n"],"metadata":{"id":"G6XXaEES6wUg"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import SimpleRNN, Dense\n","import numpy as np\n","\n","# Reshaping the data for RNN\n","# RNN expects data in the shape (number of samples, number of timesteps, number of features per timestep)\n","# Here, we treat each feature as a timestep\n","X_train_rnn = np.expand_dims(X_train_scaled, axis=1)\n","X_test_rnn = np.expand_dims(X_test_scaled, axis=1)\n","\n","# Defining the RNN model\n","rnn_model = Sequential()\n","rnn_model.add(SimpleRNN(50, input_shape=(X_train_rnn.shape[1], X_train_rnn.shape[2])))\n","rnn_model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","rnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = rnn_model.fit(X_train_rnn, y_train, epochs=100, batch_size=32, verbose=0, validation_data=(X_test_rnn, y_test))\n","\n","# Evaluate the model\n","rnn_accuracy = rnn_model.evaluate(X_test_rnn, y_test, verbose=0)[1]\n","rnn_accuracy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhyABYvj6oOR","executionInfo":{"status":"ok","timestamp":1712516148543,"user_tz":-60,"elapsed":7645,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"459a4357-fa93-4e56-fda9-94f600261514"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9230769276618958"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["##6. Long Short-Term Memory (LSTM) network (**Deep Learning**)\n"],"metadata":{"id":"thiv47Ux9YaT"}},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import LSTM, Dense\n","import numpy as np\n","\n","# Reshaping the data for LSTM\n","# LSTM expects data in the shape (number of samples, number of timesteps, number of features per timestep)\n","# Here, we treat each feature as a timestep\n","X_train_lstm = np.expand_dims(X_train_scaled, axis=1)\n","X_test_lstm = np.expand_dims(X_test_scaled, axis=1)\n","\n","# Defining the LSTM model\n","lstm_model = Sequential()\n","lstm_model.add(LSTM(50, input_shape=(X_train_lstm.shape[1], X_train_lstm.shape[2])))\n","lstm_model.add(Dense(1, activation='sigmoid'))\n","\n","# Compile the model\n","lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","history = lstm_model.fit(X_train_lstm, y_train, epochs=100, batch_size=32, verbose=0, validation_data=(X_test_lstm, y_test))\n","\n","# Evaluate the model\n","lstm_accuracy = lstm_model.evaluate(X_test_lstm, y_test, verbose=0)[1]\n","lstm_accuracy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5ySJhWm8Bqr","executionInfo":{"status":"ok","timestamp":1712516523682,"user_tz":-60,"elapsed":15196,"user":{"displayName":"Khaled Koubaa","userId":"09272044197112416538"}},"outputId":"fb14f9e4-b5cb-4c12-f662-0d3a3c4ebc77"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9230769276618958"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["##6. notes"],"metadata":{"id":"8vxCs9J37nOw"}},{"cell_type":"markdown","source":["les datasets **X_train_scaled** et **y_train** utilisés pour le training étaient les mêmes dans tous les modèles. Cela garantit une comparaison équitable des modèles, car ils ont tous appris à partir des mêmes données.\n","\n","> Add blockquote\n","\n"],"metadata":{"id":"XsFPQsO63yEx"}}]}
